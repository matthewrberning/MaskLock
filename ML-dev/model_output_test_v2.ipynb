{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process for taking an image and getting outputs from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import ImageDraw, Image\n",
    "from facenet_pytorch import MTCNN\n",
    "from torchvision import transforms\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this part is preliminary: using it to simulate a face coming from MTCNN and scaled to fit 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_image(img_path, output_image_size, face_detector):\n",
    "    image_raw = cv2.imread(img_path)\n",
    "\n",
    "    image = cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    boxes, confidence = face_detector.detect(image_raw)\n",
    "    \n",
    "    if boxes is None:\n",
    "        return None\n",
    "    bounding_box = boxes[0]\n",
    "    \n",
    "    #collect image height and width\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    #set scaling factor\n",
    "    scale=0.98\n",
    "\n",
    "    #collect coordinates of bounding box\n",
    "    x1 = bounding_box[0] #dib face.left\n",
    "    y1 = bounding_box[1] #dib face.top\n",
    "    x2 = bounding_box[2] #dib face.right\n",
    "    y2 = bounding_box[3] #dib face.bottom\n",
    "\n",
    "    #scale bounding box?\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
    "\n",
    "    #control for out of bounds, x-y top left corner\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "\n",
    "    # Check for too big bb size for given x, y\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "\n",
    "    # set up for crop with slicing\n",
    "    cropped_face = image[y1:y1 + size_bb, x1:x1 + size_bb]\n",
    "\n",
    "    resized_image = cv2.resize(cropped_face, (output_image_size, output_image_size))\n",
    "    \n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up MTCNN\n",
    "\n",
    "imagefactor = 1.05\n",
    "minsize = 35  # minimum size of face\n",
    "threshold = [0.7, 0.8, 0.8]  # three steps's threshold\n",
    "\n",
    "mtcnn = MTCNN(keep_all=True, device=device,min_face_size = minsize,thresholds= threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part takes in a model checkpoint file and an already processed image (224x224, face pulled out by MTCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(checkpoint_path):\n",
    "    '''\n",
    "    function to load the baselie model \n",
    "    simmilar to the training process\n",
    "    sets the model into eval mode as well and\n",
    "    populates the trained weights from the checkpoint file\n",
    "    '''\n",
    "    dropout=0.5\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    \n",
    "    in_features = model.fc.in_features\n",
    "#     print(f'Input feature dimensions: {in_features}')\n",
    "\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(in_features, in_features // 2),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(in_features // 2),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(in_features // 2, 2)\n",
    "    )\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def gettransforms():\n",
    "    '''\n",
    "    returns the transforms the normalize the input image\n",
    "    '''\n",
    "    pre_trained_mean, pre_trained_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    com_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=pre_trained_mean, std=pre_trained_std)\n",
    "    ])\n",
    "    return  com_transforms\n",
    "\n",
    "\n",
    "\n",
    "def process_image_through_model(input_image_of_face, model, composed_transforms):\n",
    "    '''\n",
    "    function takes in a face already sized to 224x224 and passes \n",
    "    it to the composed transforms then sends the transformed image to\n",
    "    the model which gives an output, which is then maxed and returned as\n",
    "    the predicted class (1 for Mask, 0 for No-Mask) output is a tensor\n",
    "    '''\n",
    "    normalized_image = composed_transforms(input_image_of_face)\n",
    "    normalized_image_plus_batch_dim = normalized_image.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(normalized_image_plus_batch_dim)\n",
    "    _, predicted_class = torch.max(outputs.data, 1)\n",
    "    \n",
    "    print(\"classes: Mask=1, No_Mask=0\")\n",
    "    \n",
    "    print(\"predicted class: \", predicted_class.numpy()[0])\n",
    "    \n",
    "    return predicted_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: Mask=1, No_Mask=0\n",
      "predicted class:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the checkpoint path\n",
    "checkpoint_path = r\"C:/Users/OI/Desktop/data/GWU/GWU_2020_FALL_CSCI6011_PROJECT/MaskLock\\ML-dev/checkpoints/2020-10-18-15_05_52--best_model.pth\"\n",
    "\n",
    "#load the model\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "#get the dummy image\n",
    "test_img_path = \"C:/Users/OI/Desktop/test_1.png\"\n",
    "resized_image_of_face = load_image(test_img_path, 224, mtcnn)\n",
    "\n",
    "#compose the transforms\n",
    "transform = gettransforms()\n",
    "\n",
    "#get output from the model\n",
    "process_image_through_model(resized_image_of_face, model, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
